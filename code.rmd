---
title: "718 Project"
author: "Shahzeb Naveed, Dafe Eboh"
output: word_document
        
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE)
suppressMessages(library(ggplot2))
suppressMessages(library(MASS))
suppressMessages(library(ggm))
suppressMessages(library(corrgram))
suppressMessages(library(naniar))
suppressMessages(library(e1071))
suppressMessages(library(lattice))
suppressMessages(library(plyr))
suppressMessages(library(psych))
suppressMessages(library(dplyr))
suppressMessages(library(tidyverse))
suppressMessages(library(rmdformats))
suppressMessages(library(corrplot))
suppressMessages(library(reshape))
suppressMessages(library(mlogit))
suppressMessages(library(car))
suppressMessages(library(Amelia))
library(ggcorrplot)
library(formattable)

```


```{r,warning=FALSE,message=FALSE,results="hide",echo=FALSE}

data <- read.csv("C:\\Users\\shah_\\Documents\\Waterloo\\Study\\SDA 718\\Project\\Credit Risk\\loan.csv")
```

```{r}

df <-  data %>% dplyr::select(loan_status,loan_amnt,emp_length,int_rate,grade,annual_inc,acc_now_delinq ,avg_cur_bal
,chargeoff_within_12_mths
,delinq_amnt
,dti
,mort_acc
,open_acc
,pct_tl_nvr_dlq
,percent_bc_gt_75
,pub_rec
,pub_rec_bankruptcies
,tax_liens
,total_rec_late_fee)
  
#Reducing rows for faster analysis

split_index <- sample(dim(df)[1] , 0.2*nrow(df))
df = df[split_index , ]

#str(df)


```

```{r,warning=FALSE,message=FALSE,results="hide",echo=FALSE}

na.summary <- df %>% summarise_all(funs(sum(is.na(.))))
#t(na.summary)

```


```{r,fig.width=18}



df.numeric <- df %>% keep(is.numeric) 
df.numeric.nonas <- na.omit(df.numeric)

#ggcorrplot(cor(df.numeric.nonas), p.mat = cor_pmat(df.numeric.nonas), hc.order=TRUE,method = "square",outline.col="white",ggtheme = ggplot2::theme_gray, colors = c("#6D9EC1", "white", "#E46726"),title = "Correlation Matrix")

```


## **Introduction**

Post financial crisis of 2008, a great emphasis has been laid on risk management within financial institutions to enhance transparency, consumer protection and better business decisions. To aid banks in identifying the creditworthiness of loan applicants, we apply regression modelling to predict whether a loan borrower will default or not. The Lending Club Loan Data was employed that initially contained ``r nrow(data)`` rows and ``r ncol(data)`` features. Using the given data dictionary, we hypothesized ``r ncol(df)`` variables as potential significant predictors which are tabluated below:

## **Exploratory Data Analysis**

After ensuring the data is in a Tidy form, we explore the distributions of our variables of interest.

```{r,warning=FALSE,message=FALSE,results="hide",echo=FALSE,fig.width=18,fig.height=5}

df %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value, fill = key)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram() + ggtitle("Distributions of Numeric Predictor Variables")
```


For outcome variable `default`, we used `loan_status` to combine categories of timely and late payments as "Not Default" and default and charged-off as "Default".

```{r,warning=FALSE,message=FALSE,results="hide",echo=FALSE,fig.width=18,fig.height=4}

df %>%
  select_if(negate(is.numeric)) %>% 
  gather() %>% 
  ggplot(aes(value,fill = key)) +
    facet_wrap(~ key, scales = "free") +
    geom_bar() + coord_flip() + ggtitle("Distributions of Categorical Predictor Variables")

```


```{r}

df <- df %>% dplyr::filter(emp_length != "n/a")

df <- df %>% filter(loan_status=="Charged Off" | loan_status=="Fully Paid" | loan_status=="Default" | loan_status=="Late (31-120 days)" | loan_status=="Late (16-30 days)")

df$default <- df$loan_status == "Charged Off" | df$loan_status=="Default"

#summary(df$default)

df$loan_status <- droplevels(df$loan_status)
df$emp_length <- droplevels(df$emp_length)

# Setting Baseline Levels
df$emp_length <- relevel(df$emp_length,"< 1 year")
df$grade <- relevel(df$grade,"A")
#df$home_ownership <- relevel(df$home_ownership,"NONE")


```


```{r}

#data(data)

#ggplot(df , aes(x = grade , y = int_rate , fill = grade)) + geom_boxplot() + labs(y = 'Interest Rate' , x = 'Grade')

# Takes too long

#ggplot(df , aes(x = grade , y = ..count.. , fill = factor(default))) + geom_bar() + theme(legend.title = element_blank())

# Takes too long

#ggplot(df, aes(x = annual_inc , y = loan_amnt , color = int_rate)) +
 #       geom_point(alpha = 0.5 , size = 1.5) + 
  #      geom_smooth(se = F , color = 'darkred' , method = 'loess') +
   #     xlim(c(0 , 300000)) + 
    #    labs(x = 'Annual Income' , y = 'Loan Ammount' , color = 'Interest Rate')


```

We now plot a missigness map to explore the proportion of NAs concluding that only 1% of the values are missing.
  
```{r}
missmap(df,col=c("#d32f2f", "#43A047"),fig.width=18,fig.height=3)

```

We now box-plot our numeric variables to visualize outliers. Practically speaking, the extreme points do not indicate any data entry errors as they might be just some very rich people, people who got loans on very high interest rates or people with unusual financial circumstances for example. For the purpose of generalizability, we decided not to remove these outliers and will incorporate these into our prediction models.

```{r,warning=FALSE,message=FALSE,results="hide",echo=FALSE,fig.width=18,fig.height=5}

library(reshape)
plotData <- df %>% dplyr::select(-default)
meltData <- melt(plotData)
#boxplot(data=meltData, value~variable)
myPlot <- ggplot(meltData, aes(factor(variable), value))
myPlot + geom_boxplot() + facet_wrap(~variable, scale="free") + coord_flip() + ggtitle("Boxplots of Numeric Predictor Variables")

```


```{r,warning=FALSE,message=FALSE,results="hide",echo=FALSE}

df.nonas <- na.omit(df)

split_index <- sample(dim(df.nonas)[1] , 0.8*nrow(df.nonas))
train.data.1 = df.nonas[split_index , ]
test.data.1 = df.nonas[-split_index , ]

#train.data.1 = df
#test.data.1 = df

#NOT WORKING
#library(ROSE)
#train.data.1.over <- ovun.sample(default ~ ., data=train.data.1, method="over")$data

library(caret)
train.data.1$default <- factor(train.data.1$default)
train.data.1 <- upSample(x = select(train.data.1, -default),
                     y = train.data.1$default,
                     yname = "default") %>% as_tibble()


train.data.2 <- train.data.1
test.data.2 <- test.data.1

train.data.3 <- train.data.1
test.data.3 <- test.data.1

train.data.4 <- train.data.1
test.data.4 <- test.data.1


```


A very important requirement of logistic regression is Incomplete Separation that can lead to unusually high standard errors. A 3-way crosstabulated table was drawn to make sure we have some data in every possible combination.

Incomplete Information

```{r}

# 3-Way Frequency Table
mytable <- table(train.data.1$default, train.data.1$grade, train.data.1$emp_length)
ftable(mytable)

```

For Complete Separation, we plotted every predictor against variable to visualize it. Note that complete separation may arise even when predictors do not exhibit it individually but that is beyond the scope of our project.

```{r,warning=FALSE,message=FALSE,results="hide",echo=FALSE,fig.width=18,fig.height=8}

train.data.1.nonas <- na.omit(train.data.1)
train.data.1.nonas %>% gather(-default,-loan_status, key = "var", value = "value") %>% ggplot(aes(x = value, y = default)) + geom_point() + facet_wrap(~ var, scales = "free")  + ggtitle("Plot of Outcome Variable vs Predictors (True = Default)")

```


## **Model Building**

Since `default` is a binary variable, a logistic regression model was used. But before diving into modelling, we first take a look at some of the underlying assumptions below.

#### **Assumptions of Logistic Regression**

1. **Large sample size:** ``r nrow(df)`` rows. Enough said.
2. **No or Less Multicollinearity:** We'll come to that in a while.
3. **Linearity of predictors and logit of outcome variable:** We'll come to that in a while as well.
4. **Complete Information:** As mentioned earlier, we have `0` NAs and have data for every possible combination.
5. **Incomplete Separation:** We can clearly see that there is no complete separation in the scatterplots. `Appendix (i)`

Class Imbalance: Since our data contains ``r table(df$default)[1]`` default cases and  ``r table(df$default)[2]`` not default cases, we up-sampled our minority class before training our model.


*This causes the performance of existing classifiers to get biased towards majority class.
The algorithms are accuracy driven i.e. they aim to minimize the overall error to which the minority class contributes very little. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/

While we avoid loosing information with this approach, we also run the risk of overfitting our model as we are more likely to get the same samples in the training and in the test data, i.e. the test data is no longer independent from training data. This would lead to an overestimation of our modelâ€™s performance and generalizability.

In reality though, we should not simply perform over- or under-sampling on our training data and then run the model. We need to account for cross-validation and perform over- or under-sampling on each fold independently to get an honest estimate of model performance!*


### Model Building

  
We trained 4 logistic regression models: Simple, L1-Regularized, L2-Regularized and Elastic-Net with *alpha* arbitrarily chosen mid-way between L1 and L2 as `0.5`. With regularization, the optimal penalty measure *lambda* is selected such that it minimizes the cross-validated out-of-sample accuracy error. In short, L1 forces some of the co-efficients to exactly zero thus aiding us in variable selection and model simplification. L2 forces some co-efficients close to zero while Elastic-net does some of both the worlds.

```{r}

model.1 = stats::glm(default ~ . - loan_status,  train.data.1 , family = binomial(link = 'logit'))
summary(model.1)

```

```{r}
#not working

#library(safeBinaryRegression)

#df.model.smart <- train.data.1 %>% dplyr::select(-loan_status)
#colnames(df.model.smart)


#model.1.smart =  safeBinaryRegression::glm(default ~ . ,  df.model.smart , family = binomial(link = 'logit'),separation = "test")

#summary(model.1.smart)

library(detectseparation)
library(brglm2)

#detectseparation::check_infinite_estimates(model.1)
#just prints values. Doesn't even print NAs or standard errors.


checker <- detectseparation::checkInfiniteEstimates(model.1)
plot(checker,caption="Change in estimate of Co-efficients over iterations in Training",main="Cross-validation plot of Lambad vs Binomial Deviance")

#detectseparation::detect_separation(model.1)

#brglm2::

#model.1.smart = glm(default ~ . - loan_status,  train.data.1 , family = binomial(link = 'logit'),method = "detect_separation",linear_program = "dual")
#summary(model.1.smart)


```

```{r}

#stepAIC(model.1)

```


```{r,warning=FALSE,message=FALSE,results="hide",echo=FALSE}

library(glmnet)
x <- model.matrix(default ~ . - loan_status,  train.data.2)[,-1]
y <- as.integer(train.data.2$default)

#1 for lasso, 0 for ridge, (0,1) for elastic net regression. lambda that minimize the cross-validation prediction error rate. This can be determined automatically using the function cv.glmnet()

set.seed(123) 
cv.lasso.2 <- cv.glmnet(x, y, alpha = 1, family = "binomial")
model.2 <- glmnet(x, y, alpha = 1, family = "binomial", lambda = cv.lasso.2$lambda.min, standardize = TRUE)


set.seed(223) 
cv.lasso.3 <- cv.glmnet(x, y, alpha = 0, family = "binomial")
model.3 <- glmnet(x, y, alpha = 0, family = "binomial", lambda = cv.lasso.3$lambda.min, standardize = TRUE)

set.seed(223) 
cv.lasso.4 <- cv.glmnet(x, y, alpha = 0.5, family = "binomial")
model.4 <- glmnet(x, y, alpha = 0.5, family = "binomial", lambda = cv.lasso.4$lambda.min, standardize = TRUE)

```


```{r,warning=FALSE,message=FALSE,results="hide",echo=FALSE}

plot(cv.lasso.2)
title("L1 Regularization:\nCross-validation plot of Lambad vs Binomial Deviance",line=-2.5)

plot(cv.lasso.3)
title("L2 Regularization:\nCross-validation plot of Lambad vs Binomial Deviance",line=-2.5)

plot(cv.lasso.4)
title("Elastic-net Regularization:\nCross-validation plot of Lambad vs Binomial Deviance",line=-2.5)


```


A plot visualizing the change in co-efficients as lambda is varied is shown below.

```{r,warning=FALSE,message=FALSE,results="hide",echo=FALSE}

lambdas_to_try <- 10^seq(-3, 5, length.out = 100)

# See how increasing lambda shrinks the coefficients --------------------------
# Each line shows coefficients for one variables, for different lambdas.
# The higher the lambda, the more the coefficients are shrinked towards zero.
res <- glmnet(x, y, alpha = 1, lambda = lambdas_to_try, standardize = FALSE)
plot(res, xvar = "lambda",caption="Change in Co-efficients in Elastic-net Regularization with Lambda",main="Change in Co-efficients in L1 Regularization with Lambda",)
legend("bottomright", lwd = 1, col = 1:ncol(x), legend = colnames(x), cex = .7)

res <- glmnet(x, y, alpha = 0, lambda = lambdas_to_try, standardize = FALSE)
plot(res, xvar = "lambda",caption="Change in Co-efficients in Elastic-net Regularization with Lambda",main="Change in Co-efficients in L2 Regularization with Lambda",)
legend("bottomright", lwd = 1, col = 1:ncol(x), legend = colnames(x), cex = .7)


res <- glmnet(x, y, alpha = 0.5, lambda = lambdas_to_try, standardize = FALSE)
plot(res, xvar = "lambda",caption="Change in Co-efficients in Elastic-net Regularization with Lambda",main="Change in Co-efficients in Elastic-net Regularization with Lambda",)
legend("bottomright", lwd = 1, col = 1:ncol(x), legend = colnames(x), cex = .7)


```


      
```{r}

coef(model.1)
coef(model.2)
coef(model.3)
coef(model.4)
#coef(cv.lasso, cv.lasso$lambda.min) #most accurate
#coef(cv.lasso, cv.lasso$lambda.1se) #accurate and simple

```

## Model Evaluation: Goodness-of-Fit

To evaluate model fit, the Deviance Statistics and H&L R^2 values (that can also be used as effect size) for all of our models have been compared below. We see that there is not much difference in the model fit with Model 1 doing slightly better.

```{r}

logisticPseudoR2s <- function(LogModel) {
dev <- LogModel$deviance
nullDev <- LogModel$null.deviance
modelN <- length(LogModel$fitted.values)
R.l <- 1 - dev / nullDev
#R.cs <- 1- exp ( -(nullDev - dev) / modelN)
#R.n <- R.cs / ( 1 - ( exp (-(nullDev / modelN))))

cat("Simple:        \t\t\t\t", round(R.l, 4), "\n")
#cat("Cox and Snell R^2 ", round(R.cs, 3), "\n")
#cat("Nagelkerke R^2 ", round(R.n, 3), "\n")
}

cat("Hosmer and Lemeshow R^2 \n")
logisticPseudoR2s(model.1)
R1.2 <- 1 - deviance(model.2) / model.2$nulldev
cat("L1-Regularization: \t\t\t", round(R1.2, 4), "\n")
R1.3 <- 1 - deviance(model.3) / model.3$nulldev
cat("L2-Regularization: \t\t\t", round(R1.3, 4), "\n")
R1.4 <- 1 - deviance(model.4) / model.4$nulldev
cat("Elastic-net Regularization: \t\t", round(R1.4, 4), "\n")

```

```{r}

modelChi.1 <- model.1$null.deviance - model.1$deviance  #improvement
chidf.1 <- model.1$df.null - model.1$df.residual #no. of predictors
chisq.prob.1 <- 1 - pchisq(modelChi.1, chidf.1)

modelChi.2 <- model.2$nulldev - deviance(model.2)
chidf.2 <- model.2$df
chisq.prob.2 <- 1 - pchisq(modelChi.2, chidf.2)

modelChi.3 <- model.3$nulldev - deviance(model.3)
chidf.3 <- model.3$df
chisq.prob.3 <- 1 - pchisq(modelChi.3, chidf.3)

modelChi.4 <- model.4$nulldev - deviance(model.4)
chidf.4 <- model.4$df
chisq.prob.4 <- 1 - pchisq(modelChi.4, chidf.4)

Chi.Statistic <- c(modelChi.1,modelChi.2,modelChi.3,modelChi.4)
df <- c(chidf.1,chidf.2,chidf.3,chidf.4)
p.value <- c(chisq.prob.1,chisq.prob.2,chisq.prob.3,chisq.prob.4)
R.1 <- 0.1032
R.Square <- c(R.1,R1.2,R1.3,R1.4)

Method <- c("Simple Regression","L1","L2","Elastic-net")


fit_table <- data.frame(Method,R.Square,Chi.Statistic,df,p.value)

formattable(fit_table,align = c("l",rep("r", NCOL(fit_table) - 1)), list(`Indicator Name` = formatter("span", style = ~ style(color = "black", font.weight = "bold")), `p.value` = color_bar("#FA614B"),`df` = color_bar("#0D47A1"),`Chi.Statistic` = color_bar("#9c64a6")))



```




Reduces (over)-fitting but does slighly better at out-of-sample accuracy. no?

Diagnostics:


```{r}

x.train <- model.matrix(default ~. -loan_status, train.data.2)[,-1]
train.data.2$predicted.probabilities <- predict(model.2,newx = x.train,type="response")

x.train <- model.matrix(default ~. -loan_status, train.data.3)[,-1]
train.data.3$predicted.probabilities <- predict(model.3,newx = x.train,type="response")

x.train <- model.matrix(default ~. -loan_status, train.data.4)[,-1]
train.data.4$predicted.probabilities <- predict(model.4,newx = x.train,type="response")

train.data.1$predicted.probabilities<- fitted(model.1) 
train.data.1$standardized.residuals<- rstandard(model.1) 
train.data.1$studentized.residuals<- rstudent(model.1) 
#train.data.1$dfbeta<- dfbeta(model.1) #Take Long
#train.data.1$dffit<- dffits(model.1) #Take Long
train.data.1$leverage<- hatvalues(model.1)
train.data.1$cooks.distance<- cooks.distance(model.1)

#write.table (train.data.1, "Model 1 With Diagnostics.dat", sep = "\t", row.names = FALSE)

cp <- plot(model.1, which = 4, id.n = 3,caption="Cook's Distance Plot for Simple Model",main="Cook's Distance with respect to Simple Model")

#max(train.data.1$cooks.distance)


```

To make sure none of our observations have an unfair influence on our model, we plot a Cook's distance plot using predicted probabilities from Model 1 and observe that even the most influential observation has a cook's distance of ``r max(train.data.1$cooks.distance)``


Assumptions: Coming Back

"The "generalized variance inflation factors" (GVIF) implemented in the vif() function of the R car package were designed by Fox and Monette specifically to handle situations like this, where there are groups of predictor variables that should be considered together rather than separately."

https://stats.stackexchange.com/questions/430412/vif-for-categorical-variable-with-more-than-2-categories

"The variables with high VIFs are indicator (dummy) variables that represent a categorical variable with three or more categories. If the proportion of cases in the reference category is small, the indicator variables will necessarily have high VIFs, even if the categorical variable is not associated with other variables in the regression model."

https://statisticalhorizons.com/multicollinearity


Caclulcate Average VIF


To test for multicollinearity among our predictors, we calculate GVIF and observe that none of the variables have GVIF's greater than 10 which, if adjusted for the degrees of freedom, are even less.

```{r,warning=FALSE,message=FALSE,results="hide",echo=FALSE}

vif(model.1) 

#GVIFs got inflated when complete separation occurred

```

Testing for linearity of the logit, only continous.

"After coding a bivariate categorical variable, the relationship is by definition linear.  There's no reason to check for satisfying the assumption in that case.  You only have two points to connect."

https://www.researchgate.net/post/Determining_linearity_between_the_dependent_and_independent_variable



For ensuring logit-linearity, we employ Box-Tidwell test on Model 1 and find that all the log-interaction terms are signficant indicating non-linearity. But this is highly probable because of the massive sample size that always results in small standard errors, resulting in extremely significant z-statistics. A better and an easier way it to simply plot the logit against the predictor variables. As can be seen below, the relationship can safely be approximated as linear. Note that we don't need a non-linearity test for categorical variables since they are coded as dummy variables with values 0 and 1 so the relationship becomes "linear" by definition since we have only two points to connect.



```{r}

#This contains missing variables

train.data.1$log.loan_amnt <- log(train.data.1$loan_amnt)*train.data.1$loan_amnt
train.data.1$log.int_rate <- log(train.data.1$int_rate)*train.data.1$int_rate
train.data.1$log.annual_inc <- log(train.data.1$annual_inc)*train.data.1$annual_inc

model.1.log = glm(default ~ loan_amnt+int_rate+annual_inc+log.loan_amnt+log.int_rate+log.annual_inc,  train.data.1 , family = binomial(link = 'logit'))

summary(model.1.log)

```



```{r,warning=FALSE,message=FALSE,results="hide",echo=FALSE,fig.width=18,fig.height=8}


library(ggpubr)
library(egg)
library(gridExtra)
library(grid)
library(ggplot2)
library(lattice)


train.data.1 <- train.data.1 %>% mutate(mylogit = log(predicted.probabilities/(1-predicted.probabilities))) #pred = probability
loan_amnt <- train.data.1 %>% ggplot(aes(x=loan_amnt,y=mylogit)) + geom_point(alpha=0.1)+geom_smooth()
annual_inc <- train.data.1 %>% ggplot(aes(x=annual_inc,y=mylogit))+geom_point(alpha=0.25)+geom_smooth()
int_rate <- train.data.1 %>% ggplot(aes(x=int_rate,y=mylogit))+geom_point(alpha=0.1)+geom_smooth()
acc_now_delinq <- train.data.1 %>% ggplot(aes(x=acc_now_delinq ,y=mylogit))+geom_point(alpha=0.1)+geom_smooth()
avg_cur_bal <- train.data.1 %>% ggplot(aes(x=avg_cur_bal,y=mylogit))+geom_point(alpha=0.1)+geom_smooth()
chargeoff_within_12_mths <- train.data.1 %>% ggplot(aes(x=chargeoff_within_12_mths  ,y=mylogit))+geom_point(alpha=0.1)+geom_smooth()
delinq_amnt <- train.data.1 %>% ggplot(aes(x=delinq_amnt,y=mylogit))+geom_point(alpha=0.1)+geom_smooth()
dti <- train.data.1 %>% ggplot(aes(x=dti,y=mylogit))+geom_point(alpha=0.1)+geom_smooth()
tax_liens <- train.data.1 %>% ggplot(aes(x=tax_liens,y=mylogit))+geom_point(alpha=0.1)+geom_smooth()
pub_rec_bankruptcies <- train.data.1 %>% ggplot(aes(x=pub_rec_bankruptcies,y=mylogit))+geom_point(alpha=0.1)+geom_smooth()
pub_rec <- train.data.1 %>% ggplot(aes(x=pub_rec,y=mylogit))+geom_point(alpha=0.1)+geom_smooth()
mort_acc <- train.data.1 %>% ggplot(aes(x=mort_acc,y=mylogit))+geom_point(alpha=0.1)+geom_smooth()
open_acc <- train.data.1 %>% ggplot(aes(x=open_acc,y=mylogit))+geom_point(alpha=0.1)+geom_smooth()
pct_tl_nvr_dlq <- train.data.1 %>% ggplot(aes(x=pct_tl_nvr_dlq,y=mylogit))+geom_point(alpha=0.1)+geom_smooth()
percent_bc_gt_75 <- train.data.1 %>% ggplot(aes(x=percent_bc_gt_75,y=mylogit))+geom_point(alpha=0.1)+geom_smooth()

egg::ggarrange(loan_amnt,annual_inc,int_rate,acc_now_delinq,avg_cur_bal,chargeoff_within_12_mths,delinq_amnt,dti,tax_liens,pub_rec_bankruptcies,pub_rec,mort_acc,open_acc,pct_tl_nvr_dlq,percent_bc_gt_75,top=textGrob("Plot of Logit vs Numeric Predictors", gp=gpar(fontsize=18,font=1)))

#train.data.2 <- train.data.2 %>% mutate(mylogit = log(predicted.probabilities/(1-predicted.probabilities))) #pred = probability
#train.data.2 %>% ggplot(aes(x=loan_amnt,y=mylogit)) + geom_point(alpha=0.01)+geom_smooth()
#train.data.2 %>% ggplot(aes(x=annual_inc,y=mylogit))+geom_point(alpha=0.55)+geom_smooth();
#train.data.2 %>% ggplot(aes(x=int_rate,y=mylogit))+geom_point(alpha=0.01)+geom_smooth();

```





```{r,warning=FALSE,message=FALSE,results="hide",echo=FALSE}

test.data.1$predicted.probabilities <- predict(model.1,test.data.1,type="response")

x.test <- model.matrix(default ~. - loan_status, test.data.2)[,-1]
test.data.2$predicted.probabilities <- predict(model.2,newx = x.test,type="response")

x.test <- model.matrix(default ~. - loan_status, test.data.3)[,-1]
test.data.3$predicted.probabilities <- predict(model.3,newx = x.test,type="response")

x.test <- model.matrix(default ~. - loan_status, test.data.4)[,-1]
test.data.4$predicted.probabilities <- predict(model.4,newx = x.test,type="response")

```

To visualize our models, probability distributions have been compared below.

```{r,fig.width=18,fig.height=3}

test.data.1$Algorithm <- 'Model 1'
test.data.2$Algorithm <- 'L1'
test.data.3$Algorithm <- 'L2'
test.data.4$Algorithm <- 'Elastic-net'

myDenplot <- rbind(test.data.1,test.data.2,test.data.3,test.data.4)
myDenplot %>% ggplot(aes(predicted.probabilities,fill = Algorithm))+geom_density(alpha=0.2) + ggtitle("Density Plot for Probability Distributions of Regression Models")+xlab("Probability")+ylab("Density")


#ggplot(aes(predicted.probabilities)) + geom_density(fill = 'lightblue' , alpha = 0.4)+ labs(x = 'Predicted Probabilities on #Testing Set')

#test.data.2 %>% ggplot(aes(predicted.probabilities)) + geom_density(fill = 'lightblue' , alpha = 0.4)+ labs(x = 'Predicted Probabilities on Testing Set')

#test.data.3 %>% ggplot(aes(predicted.probabilities)) + geom_density(fill = 'lightblue' , alpha = 0.4)+ labs(x = 'Predicted Probabilities on Testing Set')

#test.data.4 %>% ggplot(aes(predicted.probabilities)) + geom_density(fill = 'lightblue' , alpha = 0.4)+ labs(x = 'Predicted Probabilities on Testing Set')




```


## **Testing Prediction Accuracy**

We now evaluate our models based on out-of-sample accuracy and observe that for our data, all the 4 models designed perform more or less, in a similar fashion. Their ROC curves alongwith AUCs as well as their various evaluation metrics are calcucated below.

```{r,fig.width=12,fig.height=5,}

# k <- 0
# accuracy.1 = c()
# sensitivity.1 = c()
# specificity.1 = c()
# 
# accuracy.2 = c()
# sensitivity.2 = c()
# specificity.2 = c()
# 
# 
# accuracy.3 = c()
# sensitivity.3 = c()
# specificity.3 = c()
# 
# 
# accuracy.4 = c()
# sensitivity.4 = c()
# specificity.4 = c()
# 
# 
# max.threshold <- 0.6 #why not above 60%
# 
# for(i in seq(from = 0.1 , to = max.threshold , by = 0.05)){
#         k = k + 1
#         preds_binomial = ifelse(test.data.1$predicted.probabilities > i , 1 , 0)
#         confmat = table(test.data.1$default , preds_binomial)
#         accuracy.1[k] = sum(diag(confmat)) / sum(confmat)
#         sensitivity.1[k] = confmat[1 , 1] / sum(confmat[ , 1])
#         specificity.1[k] = confmat[2 , 2] / sum(confmat[ , 2])
# }
# 
# threshold <- seq(from = 0.1 , to = max.threshold , by = 0.05)
# 
# cutoff.1 <- data.frame(threshold, accuracy.1, sensitivity.1, specificity.1)
# 
# # Gather accuracy , sensitivity and specificity in one column
# #ggplot(gather(cutoff , key = 'Metric' , value = 'Value' , 2:4) , 
#  #      aes(x = threshold , y = Value , color = Metric)) + 
#   #      geom_line(size = 1.5)
# 
# k <- 0
# 
# for(i in seq(from = 0.1 , to = max.threshold , by = 0.05)){
#         k = k + 1
#         preds_binomial = ifelse(test.data.2$predicted.probabilities > i , 1 , 0)
#         confmat = table(test.data.2$default , preds_binomial)
#         accuracy.2[k] = sum(diag(confmat)) / sum(confmat)
#         sensitivity.2[k] = confmat[1 , 1] / sum(confmat[ , 1])
#         specificity.2[k] = confmat[2 , 2] / sum(confmat[ , 2])
# }
# 
# cutoff.2 <- data.frame(accuracy.2, sensitivity.2, specificity.2)
# 
# # Gather accuracy , sensitivity and specificity in one column
# #ggplot(gather(cutoff , key = 'Metric' , value = 'Value' , 2:4) , 
#  #      aes(x = threshold , y = Value , color = Metric)) + 
#   #      geom_line(size = 1.5)
# 
# k <- 0
# 
# for(i in seq(from = 0.1 , to = max.threshold , by = 0.05)){
#         k = k + 1
#         preds_binomial = ifelse(test.data.3$predicted.probabilities > i , 1 , 0)
#         confmat = table(test.data.3$default , preds_binomial)
#         accuracy.3[k] = sum(diag(confmat)) / sum(confmat)
#         sensitivity.3[k] = confmat[1 , 1] / sum(confmat[ , 1])
#         specificity.3[k] = confmat[2 , 2] / sum(confmat[ , 2])
# }
# 
# cutoff.3 <- data.frame(accuracy.3, sensitivity.3, specificity.3)
# 
# # Gather accuracy , sensitivity and specificity in one column
# #ggplot(gather(cutoff , key = 'Metric' , value = 'Value' , 2:4) , 
#  #      aes(x = threshold , y = Value , color = Metric)) + 
#   #      geom_line(size = 1.5)
# 
# k <- 0
# 
# for(i in seq(from = 0.1 , to = max.threshold , by = 0.05)){
#         k = k + 1
#         preds_binomial = ifelse(test.data.4$predicted.probabilities > i , 1 , 0)
#         confmat = table(test.data.4$default , preds_binomial)
#         accuracy.4[k] = sum(diag(confmat)) / sum(confmat)
#         sensitivity.4[k] = confmat[1 , 1] / sum(confmat[ , 1])
#         specificity.4[k] = confmat[2 , 2] / sum(confmat[ , 2])
# }
# 
# cutoff.4 <- data.frame(accuracy.4, sensitivity.4, specificity.4)
# 
# 
# # Gather accuracy , sensitivity and specificity in one column
# #ggplot(gather(cutoff , key = 'Metric' , value = 'Value' , 2:4) , 
#  #      aes(x = threshold , y = Value , color = Metric)) + 
#   #      geom_line(size = 1)
# 
# 
# #cutoff.1$identi <- 'Simple'
# #cutoff.2$identi <- 'L1'
# #cutoff.3$identi <- 'L2'
# #cutoff.4$identi <- 'Elastic-net'
# 
# 
# myaccuplot <- cbind(cutoff.1,cutoff.2,cutoff.3,cutoff.4)
# head(myaccuplot)
# 
# #myaccuplot %>% gather(key = 'Metric',value='Value') %>% ggplot(aes(threshold,Value,color=Metric)) + geom_line()
# mysize <- 1.2
# 
# #ggplot(data=myaccuplot)+
#  # geom_line(aes(threshold,accuracy.1),color="blue",size=mysize)+
#   #geom_line(aes(threshold,accuracy.2),color="purple",size=mysize)+
#   #geom_line(aes(threshold,accuracy.3),color="orange",size=mysize)+
#   #geom_line(aes(threshold,accuracy.4),color="red",size=mysize)+
#   #geom_line(aes(threshold,sensitivity.1),color="blue",linetype="dashed",size=mysize)+
#   #geom_line(aes(threshold,sensitivity.2),color="purple",linetype="dashed",size=mysize)+
#   #geom_line(aes(threshold,sensitivity.3),color="orange",linetype="dashed",size=mysize)+
#   #geom_line(aes(threshold,sensitivity.4),color="red",linetype="dashed",size=mysize)+
#   #geom_line(aes(threshold,specificity.1),color="blue",linetype="dotted",size=mysize)+
#   #geom_line(aes(threshold,specificity.2),color="purple",linetype="dotted",size=mysize)+
#   #geom_line(aes(threshold,specificity.3),color="orange",linetype="dotted",size=mysize)+
#   #geom_line(aes(threshold,specificity.4),color="red",linetype="dotted",size=mysize)+
#   #xlab("Time") +
#   #ylab("value") +
#   #scale_colour_manual("", breaks = c("x", "y", "z"), values = c("red", "blue", "green"))
# 

```


```{r}

cut_prob <- 0.2

test.data.1$predicted.classes = ifelse(test.data.1$predicted.probabilities > cut_prob , 1 , 0)
test.data.2$predicted.classes = ifelse(test.data.2$predicted.probabilities > cut_prob , 1 , 0)
test.data.3$predicted.classes = ifelse(test.data.3$predicted.probabilities > cut_prob , 1 , 0)
test.data.4$predicted.classes = ifelse(test.data.4$predicted.probabilities > cut_prob , 1 , 0)


#conf.mat.2 = table(Predicted = test.data.2$predicted.classes , Actual = test.data.2$default)
#conf.mat.2

#conf.mat.3 = table(Predicted = test.data.3$predicted.classes , Actual = test.data.3$default)
#conf.mat.3

#conf.mat.4 = table(Predicted = test.data.4$predicted.classes , Actual = test.data.4$default)
#conf.mat.4


```


```{r,warning=FALSE,message=FALSE,results="hide",echo=FALSE}

library(PRROC)
PRROC_obj.1 <- roc.curve(scores.class0 = test.data.1$predicted.probabilities, weights.class0=test.data.1$default,curve=TRUE)
plot(PRROC_obj.1,main="ROC Curve for Simple Logistic Regression")

PRROC_obj.2 <- roc.curve(scores.class0 = test.data.2$predicted.probabilities, weights.class0=test.data.2$default,curve=TRUE)
plot(PRROC_obj.2,main="ROC Curve for L1 Regularization")

PRROC_obj.3 <- roc.curve(scores.class0 = test.data.3$predicted.probabilities, weights.class0=test.data.3$default,curve=TRUE)
plot(PRROC_obj.3,main="ROC Curve for L2 Regularization")

PRROC_obj.4 <- roc.curve(scores.class0 = test.data.4$predicted.probabilities, weights.class0=test.data.4$default,curve=TRUE)
plot(PRROC_obj.4,main="ROC Curve for Elastic-net Regularization")


```


```{r}
#library(plotROC)
#rocplot.1 <- ggplot(test.data.1, aes(m = predicted.probabilities, d = default))+ geom_roc(n.cuts=10,labels=FALSE,show.legend = TRUE)

#rocplot.1 + geom_rocci(fill="pink")+ style_roc(theme = theme_grey,xlab = "False Positive Rate (1-Specificity)",ylab = "True Positive Rate (Sensitivity)") 


#rocplot.2 <- ggplot(test.data.2, aes(m = predicted.probabilities, d = default))+ geom_roc(n.cuts=10,labels=FALSE,show.legend = TRUE)

#rocplot.2 + geom_rocci(fill="pink")+ style_roc(theme = theme_grey,xlab = "False Positive Rate (1-Specificity)",ylab = "True Positive Rate (Sensitivity)") 

```


```{r}



test.data.1$default <- as.integer(test.data.1$default)
test.data.2$default <- as.integer(test.data.2$default)
test.data.3$default <- as.integer(test.data.3$default)
test.data.4$default <- as.integer(test.data.4$default)

confusionMatrix(factor(test.data.1$predicted.classes), factor(test.data.1$default), mode = "prec_recall", positive="1")
#confusionMatrix(factor(test.data.2$predicted.classes), factor(test.data.2$default), mode = "prec_recall", positive="1")
#confusionMatrix(factor(test.data.3$predicted.classes), factor(test.data.3$default), mode = "prec_recall", positive="1")
#confusionMatrix(factor(test.data.4$predicted.classes), factor(test.data.4$default), mode = "prec_recall", positive="1")


```

```{r}

#table(test.data.1$predicted.classes)
#table(test.data.1$default)


#summary(test.data.1$predicted.classes)
#summary(test.data.1$default)

```



```{r}

#test.data.1$predicted.classes <- ifelse(test.data.1$predicted.classes == TRUE,1,0)
#test.data.1$default <- ifelse(test.data.1$default == TRUE,1,0)

# Model accuracy
ac.1 <- mean(test.data.1$predicted.classes == test.data.1$default)
ac.2 <- mean(test.data.2$predicted.classes == test.data.2$default)
ac.3 <- mean(test.data.3$predicted.classes == test.data.3$default)
ac.4 <- mean(test.data.4$predicted.classes == test.data.4$default)



precision.1 <- posPredValue(factor(test.data.1$predicted.classes), factor(test.data.1$default), positive ="1")
recall.1 <- specificity(factor(test.data.1$predicted.classes), factor(test.data.1$default))
f1.1 <- (2 * precision.1 * recall.1) / (precision.1 + recall.1)

precision.2 <- posPredValue(factor(test.data.2$predicted.classes), factor(test.data.2$default), positive ="1")
recall.2 <- specificity(factor(test.data.2$predicted.classes), factor(test.data.2$default))
f1.2 <- (2 * precision.2 * recall.2) / (precision.2 + recall.2)

precision.3 <- posPredValue(factor(test.data.3$predicted.classes), factor(test.data.3$default), positive ="1")
recall.3 <- specificity(factor(test.data.3$predicted.classes), factor(test.data.3$default))
f1.3 <- (2 * precision.3 * recall.3) / (precision.3 + recall.3)

precision.4 <- posPredValue(factor(test.data.4$predicted.classes), factor(test.data.4$default), positive ="1")
recall.4 <- specificity(factor(test.data.4$predicted.classes), factor(test.data.4$default))
f1.4 <- (2 * precision.4 * recall.4) / (precision.4 + recall.4)


Method <- c("Simple Regression","L1","L2","Elastic-net")
Precision <- c(precision.1,precision.2,precision.3,precision.4)
Recall <- c(recall.1,recall.2,recall.3,recall.4)
F1.Score <- c(f1.1,f1.2,f1.3,f1.4)
Accuracy <- c(ac.1,ac.2,ac.3,ac.4)

Accu_Table <- data.frame(Method,Accuracy, Precision,Recall,F1.Score)

library(formattable)

formattable(Accu_Table,align = c("l",rep("r", NCOL(Accu_Table) - 1)), list(`Indicator Name` = formatter("span", style = ~ style(color = "black", font.weight = "bold")), `Accuracy` = color_bar("#FA614B"), `Precision` = color_bar("#33691E"),`Recall` = color_bar("#0D47A1"),`F1.Score` = color_bar("#9c64a6")))


```

## **Gap Analysis and Future Work**

For future work, outlier observations may be considered to be removed to evaluate their performance on out-of-sample observations. Furthermore, *alpha* may be calculcated using cross-validation from `caret` package for a more accurate Elastic-Net models. Moreover, new variables might be included in the model after careful study from the data dictionary.

## **Conclusion**

To conclude, regardless of the levels of regularization, logistic regression gives us a similar performance. For our (slightly) best performing model, Model 1, the Deviance Statistic was ``r round(modelChi.1,2)`` and an out-of-sample Accuracy rate of ``r mean(test.data.1$predicted.classes == test.data.1$default)`` with other models performing similarly. While using L1 regularization, variables were removed from the model reducing complexity. L2 regularization had slightly poor performance while Elastic-net removed variables.


```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

